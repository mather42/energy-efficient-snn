{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Surrogate Gradient SNN Training with Complete Energy Tracking\n",
        "\n",
        "This notebook trains a Surrogate Gradient SNN on MNIST and saves **all metrics needed for energy analysis**:\n",
        "\n",
        "## What Gets Saved\n",
        "\n",
        "| Category | Metrics | Used For |\n",
        "|----------|---------|----------|\n",
        "| **Training Spikes** | total_input, total_hidden, total_output | Training energy analysis |\n",
        "| **Inference Spikes** | input/hidden/output per inference | Inference energy analysis |\n",
        "| **Connectivity** | fanout, synapse counts | Synaptic event calculation |\n",
        "| **Performance** | accuracy, training time | Efficiency comparisons |\n",
        "\n",
        "## Workflow\n",
        "1. Run all cells in order\n",
        "2. Training automatically tracks all spikes\n",
        "3. After training, inference spikes are measured on test data\n",
        "4. Complete checkpoint saved with all energy-relevant data"
      ],
      "metadata": {
        "id": "header_cell"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_1_imports"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Cell 1: Imports and Setup\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\nSetup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 2: LIF Neuron with Spike Counting\n",
        "# =============================================================================\n",
        "\n",
        "class SimpleLIFNeuron(nn.Module):\n",
        "    \"\"\"\n",
        "    Leaky Integrate-and-Fire neuron with surrogate gradient.\n",
        "    Includes spike counting for energy analysis.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, output_size, threshold=1.0, decay=0.9):\n",
        "        super().__init__()\n",
        "        self.synapses = nn.Linear(input_size, output_size)\n",
        "        self.decay = decay\n",
        "        self.threshold = threshold\n",
        "        self.membrane_v = None\n",
        "\n",
        "        # Spike counting for energy analysis\n",
        "        self.total_spikes = 0\n",
        "        self.counting_enabled = True\n",
        "\n",
        "    def forward(self, input_spikes):\n",
        "        batch_size = input_spikes.size(0)\n",
        "        if self.membrane_v is None or self.membrane_v.size(0) != batch_size:\n",
        "            self.membrane_v = torch.zeros(\n",
        "                batch_size, self.synapses.out_features,\n",
        "                device=input_spikes.device\n",
        "            )\n",
        "\n",
        "        # Integrate\n",
        "        synaptic_current = self.synapses(input_spikes)\n",
        "        self.membrane_v = self.decay * self.membrane_v + synaptic_current\n",
        "\n",
        "        # Spike with surrogate gradient (straight-through estimator)\n",
        "        spikes = (self.membrane_v >= self.threshold).float()\n",
        "        spikes = spikes + (self.membrane_v - self.membrane_v.detach()) * 0.3\n",
        "\n",
        "        # Count spikes for energy tracking\n",
        "        if self.counting_enabled:\n",
        "            self.total_spikes += spikes.detach().sum().item()\n",
        "\n",
        "        # Reset neurons that spiked\n",
        "        self.membrane_v = self.membrane_v * (1 - spikes.detach())\n",
        "\n",
        "        return spikes\n",
        "\n",
        "    def reset_state(self):\n",
        "        \"\"\"Reset membrane potential (call between samples).\"\"\"\n",
        "        self.membrane_v = None\n",
        "\n",
        "    def reset_spike_count(self):\n",
        "        \"\"\"Reset spike counter.\"\"\"\n",
        "        self.total_spikes = 0\n",
        "\n",
        "\n",
        "class SimpleSpikingNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Two-layer SNN: Input -> Hidden (LIF) -> Output (LIF)\n",
        "    Tracks spikes at all layers for energy analysis.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_input=784, n_hidden=400, n_output=10,\n",
        "                 threshold=1.0, decay=0.9):\n",
        "        super().__init__()\n",
        "        self.n_input = n_input\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "\n",
        "        self.layer1 = SimpleLIFNeuron(n_input, n_hidden, threshold, decay)\n",
        "        self.layer2 = SimpleLIFNeuron(n_hidden, n_output, threshold, decay)\n",
        "\n",
        "        # Input spike tracking\n",
        "        self.total_input_spikes = 0\n",
        "        self.counting_enabled = True\n",
        "\n",
        "    def forward(self, spike_sequence):\n",
        "        \"\"\"\n",
        "        Forward pass through time.\n",
        "\n",
        "        Args:\n",
        "            spike_sequence: (batch, time_steps, n_input) tensor of input spikes\n",
        "\n",
        "        Returns:\n",
        "            total_output_spikes: (batch, n_output) accumulated output spikes\n",
        "        \"\"\"\n",
        "        batch_size, time_steps, _ = spike_sequence.shape\n",
        "\n",
        "        # Reset neuron states\n",
        "        self.layer1.reset_state()\n",
        "        self.layer2.reset_state()\n",
        "\n",
        "        total_output_spikes = torch.zeros(\n",
        "            batch_size, self.n_output,\n",
        "            device=spike_sequence.device\n",
        "        )\n",
        "\n",
        "        for t in range(time_steps):\n",
        "            current_input = spike_sequence[:, t, :]\n",
        "\n",
        "            # Count input spikes\n",
        "            if self.counting_enabled:\n",
        "                self.total_input_spikes += current_input.sum().item()\n",
        "\n",
        "            # Forward through layers\n",
        "            hidden_spikes = self.layer1(current_input)\n",
        "            output_spikes = self.layer2(hidden_spikes)\n",
        "            total_output_spikes += output_spikes\n",
        "\n",
        "        return total_output_spikes\n",
        "\n",
        "    def reset_all_spike_counts(self):\n",
        "        \"\"\"Reset all spike counters.\"\"\"\n",
        "        self.total_input_spikes = 0\n",
        "        self.layer1.reset_spike_count()\n",
        "        self.layer2.reset_spike_count()\n",
        "\n",
        "    def set_counting(self, enabled):\n",
        "        \"\"\"Enable/disable spike counting.\"\"\"\n",
        "        self.counting_enabled = enabled\n",
        "        self.layer1.counting_enabled = enabled\n",
        "        self.layer2.counting_enabled = enabled\n",
        "\n",
        "    def get_spike_counts(self):\n",
        "        \"\"\"Get current spike counts.\"\"\"\n",
        "        return {\n",
        "            'input': self.total_input_spikes,\n",
        "            'hidden': self.layer1.total_spikes,\n",
        "            'output': self.layer2.total_spikes,\n",
        "        }\n",
        "\n",
        "    def get_architecture_info(self):\n",
        "        \"\"\"Get architecture details for energy analysis.\"\"\"\n",
        "        return {\n",
        "            'n_input': self.n_input,\n",
        "            'n_hidden': self.n_hidden,\n",
        "            'n_output': self.n_output,\n",
        "            'n_syn_input_hidden': self.n_input * self.n_hidden,\n",
        "            'n_syn_hidden_output': self.n_hidden * self.n_output,\n",
        "            'n_synapses_total': self.n_input * self.n_hidden + self.n_hidden * self.n_output,\n",
        "            'fanout_input_to_hidden': self.n_hidden,\n",
        "            'fanout_hidden_to_output': self.n_output,\n",
        "            'threshold': self.layer1.threshold,\n",
        "            'decay': self.layer1.decay,\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"Network classes defined!\")\n",
        "print(f\"  SimpleLIFNeuron: LIF with surrogate gradient + spike counting\")\n",
        "print(f\"  SimpleSpikingNetwork: 2-layer SNN with full spike tracking\")"
      ],
      "metadata": {
        "id": "cell_2_network"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 3: Data Loading and Encoding\n",
        "# =============================================================================\n",
        "\n",
        "def poisson_encoder(image, time_window, max_rate=80):\n",
        "    \"\"\"\n",
        "    Convert image to Poisson spike train.\n",
        "\n",
        "    Args:\n",
        "        image: (C, H, W) or flattened image tensor\n",
        "        time_window: Number of time steps\n",
        "        max_rate: Maximum firing rate in Hz\n",
        "\n",
        "    Returns:\n",
        "        spikes: (time_window, n_pixels) binary spike tensor\n",
        "    \"\"\"\n",
        "    if torch.is_tensor(image):\n",
        "        flat_image = image.cpu().reshape(-1).numpy()\n",
        "    else:\n",
        "        flat_image = np.array(image).reshape(-1)\n",
        "\n",
        "    # Convert pixel intensity to firing probability per timestep\n",
        "    # P(spike) = (intensity * max_rate) / 1000 per ms\n",
        "    firing_rates = flat_image * max_rate\n",
        "    spike_probs = firing_rates / 1000.0\n",
        "\n",
        "    # Generate spikes\n",
        "    spikes = np.random.rand(time_window, len(firing_rates)) < spike_probs\n",
        "    return torch.from_numpy(spikes.astype(np.float32))\n",
        "\n",
        "\n",
        "def prepare_spike_batch(images, time_window, max_rate=80):\n",
        "    \"\"\"\n",
        "    Convert batch of images to spike trains.\n",
        "\n",
        "    Args:\n",
        "        images: (batch, C, H, W) image batch\n",
        "        time_window: Number of time steps\n",
        "        max_rate: Maximum firing rate in Hz\n",
        "\n",
        "    Returns:\n",
        "        spike_batch: (batch, time_window, n_pixels) spike tensor\n",
        "    \"\"\"\n",
        "    batch = []\n",
        "    for img in images:\n",
        "        spikes = poisson_encoder(img.squeeze(), time_window, max_rate)\n",
        "        batch.append(spikes)\n",
        "    return torch.stack(batch)\n",
        "\n",
        "\n",
        "# Load MNIST\n",
        "transform = transforms.ToTensor()\n",
        "train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "print(f\"MNIST loaded: {len(train_data)} train, {len(test_data)} test\")\n",
        "print(f\"Image shape: {train_data[0][0].shape}\")"
      ],
      "metadata": {
        "id": "cell_3_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 4: Hyperparameters\n",
        "# =============================================================================\n",
        "\n",
        "# Architecture\n",
        "N_INPUT = 784\n",
        "N_HIDDEN = 400\n",
        "N_OUTPUT = 10\n",
        "\n",
        "# Neuron parameters\n",
        "THRESHOLD = 1.0\n",
        "DECAY = 0.9\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 64\n",
        "MAX_EPOCHS = 20\n",
        "LEARNING_RATE = 0.003\n",
        "WEIGHT_DECAY = 1e-5\n",
        "PATIENCE = 5\n",
        "\n",
        "# Encoding\n",
        "TIME_WINDOW = 50  # ms (timesteps)\n",
        "MAX_RATE = 80     # Hz\n",
        "\n",
        "# Inference measurement\n",
        "N_INFERENCE_SAMPLES = 4096\n",
        "\n",
        "# Checkpoint path\n",
        "CHECKPOINT_PATH = \"surrogate_snn_checkpoint.pth\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"HYPERPARAMETERS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nArchitecture: {N_INPUT} → {N_HIDDEN} → {N_OUTPUT}\")\n",
        "print(f\"Synapses: {N_INPUT * N_HIDDEN + N_HIDDEN * N_OUTPUT:,}\")\n",
        "print(f\"\\nNeuron: threshold={THRESHOLD}, decay={DECAY}\")\n",
        "print(f\"Encoding: {TIME_WINDOW}ms window, {MAX_RATE}Hz max rate\")\n",
        "print(f\"\\nTraining: {MAX_EPOCHS} epochs, batch={BATCH_SIZE}, LR={LEARNING_RATE}\")\n",
        "print(f\"Early stopping patience: {PATIENCE}\")"
      ],
      "metadata": {
        "id": "cell_4_hyperparams"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 5: Training\n",
        "# =============================================================================\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Create network\n",
        "snn = SimpleSpikingNetwork(\n",
        "    n_input=N_INPUT,\n",
        "    n_hidden=N_HIDDEN,\n",
        "    n_output=N_OUTPUT,\n",
        "    threshold=THRESHOLD,\n",
        "    decay=DECAY\n",
        ").to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(snn.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=2\n",
        ")\n",
        "\n",
        "# Training tracking\n",
        "best_val_acc = 0\n",
        "best_model_state = None\n",
        "epochs_without_improvement = 0\n",
        "training_history = {\n",
        "    'losses': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'epoch_times': [],\n",
        "}\n",
        "\n",
        "# Reset spike counters before training\n",
        "snn.reset_all_spike_counts()\n",
        "snn.set_counting(True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING (with spike tracking)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "training_start = time.time()\n",
        "total_training_samples = 0\n",
        "\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "    snn.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        labels = labels.to(device)\n",
        "        spike_trains = prepare_spike_batch(images, TIME_WINDOW, MAX_RATE).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = snn(spike_trains)  # Spikes counted here!\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(snn.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total_training_samples += labels.size(0)\n",
        "\n",
        "        if batch_idx % 200 == 0:\n",
        "            print(f\"  Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}: \"\n",
        "                  f\"Loss={loss.item():.4f}, Acc={100*correct/total:.1f}%\")\n",
        "\n",
        "    # Validation (don't count these spikes as training)\n",
        "    snn.eval()\n",
        "    snn.set_counting(False)\n",
        "\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            labels = labels.to(device)\n",
        "            spike_trains = prepare_spike_batch(images, TIME_WINDOW, MAX_RATE).to(device)\n",
        "            outputs = snn(spike_trains)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    snn.set_counting(True)  # Re-enable for next epoch\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    training_history['losses'].append(epoch_loss / len(train_loader))\n",
        "    training_history['train_acc'].append(train_acc)\n",
        "    training_history['val_acc'].append(val_acc)\n",
        "    training_history['epoch_times'].append(epoch_time)\n",
        "\n",
        "    print(f\"\\n*** Epoch {epoch+1}: Train={train_acc:.1f}%, Val={val_acc:.1f}%, \"\n",
        "          f\"Time={epoch_time:.1f}s ***\\n\")\n",
        "\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = {k: v.cpu().clone() for k, v in snn.state_dict().items()}\n",
        "        epochs_without_improvement = 0\n",
        "        print(f\"  ★ New best: {best_val_acc:.2f}%\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "    if epochs_without_improvement >= PATIENCE:\n",
        "        print(f\"\\nEarly stopping after {epoch+1} epochs\")\n",
        "        break\n",
        "\n",
        "    if val_acc >= 98.0:\n",
        "        print(f\"\\nReached 98%+, stopping\")\n",
        "        break\n",
        "\n",
        "training_time = time.time() - training_start\n",
        "epochs_trained = len(training_history['losses'])\n",
        "\n",
        "# Get training spike counts\n",
        "training_spikes = snn.get_spike_counts()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  Total samples processed: {total_training_samples:,}\")\n",
        "print(f\"  Total time: {training_time:.1f}s ({training_time/60:.1f} min)\")\n",
        "print(f\"  Epochs: {epochs_trained}\")\n",
        "print(f\"  Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"\\n  TRAINING SPIKES (accumulated during forward passes):\")\n",
        "print(f\"    Input:  {training_spikes['input']:,.0f} \"\n",
        "      f\"({training_spikes['input']/total_training_samples:.1f}/sample)\")\n",
        "print(f\"    Hidden: {training_spikes['hidden']:,.0f} \"\n",
        "      f\"({training_spikes['hidden']/total_training_samples:.1f}/sample)\")\n",
        "print(f\"    Output: {training_spikes['output']:,.0f} \"\n",
        "      f\"({training_spikes['output']/total_training_samples:.1f}/sample)\")"
      ],
      "metadata": {
        "id": "cell_5_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 6: Load Best Model and Measure Inference Spikes\n",
        "# =============================================================================\n",
        "\n",
        "# Load best model\n",
        "snn.load_state_dict(best_model_state)\n",
        "snn.eval()\n",
        "\n",
        "# Reset counters for inference measurement\n",
        "snn.reset_all_spike_counts()\n",
        "snn.set_counting(True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"MEASURING INFERENCE SPIKES ({N_INFERENCE_SAMPLES} test samples)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "inference_start = time.time()\n",
        "inference_samples_seen = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "        if inference_samples_seen >= N_INFERENCE_SAMPLES:\n",
        "            break\n",
        "\n",
        "        # Clip batch if needed\n",
        "        remaining = N_INFERENCE_SAMPLES - inference_samples_seen\n",
        "        if images.size(0) > remaining:\n",
        "            images = images[:remaining]\n",
        "\n",
        "        spike_trains = prepare_spike_batch(images, TIME_WINDOW, MAX_RATE).to(device)\n",
        "        outputs = snn(spike_trains)  # Spikes counted\n",
        "        inference_samples_seen += images.size(0)\n",
        "\n",
        "        if (batch_idx + 1) % 20 == 0:\n",
        "            print(f\"  [{inference_samples_seen}/{N_INFERENCE_SAMPLES}]\")\n",
        "\n",
        "inference_time = time.time() - inference_start\n",
        "inference_spikes = snn.get_spike_counts()\n",
        "\n",
        "print(f\"\\n  Measurement complete in {inference_time:.1f}s\")\n",
        "print(f\"\\n  INFERENCE SPIKES (per inference):\")\n",
        "print(f\"    Input:  {inference_spikes['input']/inference_samples_seen:.2f}\")\n",
        "print(f\"    Hidden: {inference_spikes['hidden']/inference_samples_seen:.2f}\")\n",
        "print(f\"    Output: {inference_spikes['output']/inference_samples_seen:.2f}\")"
      ],
      "metadata": {
        "id": "cell_6_inference"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 7: Final Evaluation and Visualization\n",
        "# =============================================================================\n",
        "\n",
        "# Disable counting for final evaluation\n",
        "snn.set_counting(False)\n",
        "snn.eval()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL EVALUATION\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        labels_dev = labels.to(device)\n",
        "        spike_trains = prepare_spike_batch(images, TIME_WINDOW, MAX_RATE).to(device)\n",
        "        outputs = snn(spike_trains)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "test_accuracy = 100 * np.mean(all_predictions == all_labels)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "# Per-class accuracy\n",
        "print(f\"\\nPer-class accuracy:\")\n",
        "for digit in range(10):\n",
        "    mask = all_labels == digit\n",
        "    class_acc = 100 * np.mean(all_predictions[mask] == digit)\n",
        "    print(f\"  Digit {digit}: {class_acc:.1f}%\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=range(10), yticklabels=range(10))\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.ylabel('True', fontsize=12)\n",
        "plt.title(f'Confusion Matrix (Accuracy: {test_accuracy:.2f}%)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cell_7_evaluation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 8: Training Curves\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss curve\n",
        "axes[0].plot(training_history['losses'], 'b-', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training Loss')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curves\n",
        "axes[1].plot(training_history['train_acc'], 'b-', label='Train', linewidth=2)\n",
        "axes[1].plot(training_history['val_acc'], 'r-', label='Validation', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Training & Validation Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal train accuracy: {training_history['train_acc'][-1]:.2f}%\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"Final test accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "cell_8_curves"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 9: Save Complete Checkpoint\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SAVING COMPLETE CHECKPOINT\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Get architecture info\n",
        "arch_info = snn.get_architecture_info()\n",
        "\n",
        "# Compute derived metrics for energy analysis\n",
        "n_synapses = arch_info['n_synapses_total']\n",
        "dense_ops_per_inference = n_synapses * TIME_WINDOW\n",
        "\n",
        "# Event-driven synaptic events per inference\n",
        "# Input->Hidden: input_spikes * fanout_to_hidden\n",
        "# Hidden->Output: hidden_spikes * fanout_to_output\n",
        "event_syn_per_inference = (\n",
        "    (inference_spikes['input'] / inference_samples_seen) * arch_info['fanout_input_to_hidden'] +\n",
        "    (inference_spikes['hidden'] / inference_samples_seen) * arch_info['fanout_hidden_to_output']\n",
        ")\n",
        "\n",
        "# Build checkpoint\n",
        "checkpoint = {\n",
        "    # Metadata\n",
        "    \"schema_version\": \"surrogate_complete_v1\",\n",
        "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"model_class\": \"SimpleSpikingNetwork\",\n",
        "\n",
        "    # Model weights\n",
        "    \"model_state_dict\": best_model_state,\n",
        "\n",
        "    # Architecture\n",
        "    \"architecture\": arch_info,\n",
        "    \"n_input\": N_INPUT,\n",
        "    \"n_hidden\": N_HIDDEN,\n",
        "    \"n_output\": N_OUTPUT,\n",
        "\n",
        "    # Encoding protocol\n",
        "    \"encoding\": {\n",
        "        \"type\": \"poisson\",\n",
        "        \"time_window\": TIME_WINDOW,\n",
        "        \"max_rate_hz\": MAX_RATE,\n",
        "        \"spike_prob_formula\": \"p = (pixel * max_rate) / 1000 per timestep\",\n",
        "    },\n",
        "\n",
        "    # Training configuration\n",
        "    \"training_config\": {\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"max_epochs\": MAX_EPOCHS,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"patience\": PATIENCE,\n",
        "        \"grad_clip_max_norm\": 1.0,\n",
        "    },\n",
        "\n",
        "    # Training results\n",
        "    \"training_results\": {\n",
        "        \"epochs_trained\": epochs_trained,\n",
        "        \"total_samples\": total_training_samples,\n",
        "        \"training_time_seconds\": training_time,\n",
        "        \"best_val_accuracy\": best_val_acc,\n",
        "        \"final_train_accuracy\": training_history['train_acc'][-1],\n",
        "    },\n",
        "    \"training_history\": training_history,\n",
        "\n",
        "    # Test performance\n",
        "    \"test_accuracy\": test_accuracy,\n",
        "\n",
        "    # =================================================================\n",
        "    # ENERGY ANALYSIS DATA\n",
        "    # =================================================================\n",
        "\n",
        "    # Training spikes (for training energy analysis)\n",
        "    \"training_spikes\": {\n",
        "        \"total_input\": training_spikes['input'],\n",
        "        \"total_hidden\": training_spikes['hidden'],\n",
        "        \"total_output\": training_spikes['output'],\n",
        "        \"input_per_sample\": training_spikes['input'] / total_training_samples,\n",
        "        \"hidden_per_sample\": training_spikes['hidden'] / total_training_samples,\n",
        "        \"output_per_sample\": training_spikes['output'] / total_training_samples,\n",
        "        \"total_samples\": total_training_samples,\n",
        "    },\n",
        "\n",
        "    # Inference spikes (for inference energy analysis)\n",
        "    \"inference_spikes\": {\n",
        "        \"n_samples\": inference_samples_seen,\n",
        "        \"total_input\": inference_spikes['input'],\n",
        "        \"total_hidden\": inference_spikes['hidden'],\n",
        "        \"total_output\": inference_spikes['output'],\n",
        "        \"input_per_inference\": inference_spikes['input'] / inference_samples_seen,\n",
        "        \"hidden_per_inference\": inference_spikes['hidden'] / inference_samples_seen,\n",
        "        \"output_per_inference\": inference_spikes['output'] / inference_samples_seen,\n",
        "        \"measurement_time_seconds\": inference_time,\n",
        "    },\n",
        "\n",
        "    # Connectivity (for synaptic event calculations)\n",
        "    \"connectivity\": {\n",
        "        \"n_syn_input_hidden\": arch_info['n_syn_input_hidden'],\n",
        "        \"n_syn_hidden_output\": arch_info['n_syn_hidden_output'],\n",
        "        \"n_synapses_total\": n_synapses,\n",
        "        \"fanout_input_to_hidden\": arch_info['fanout_input_to_hidden'],\n",
        "        \"fanout_hidden_to_output\": arch_info['fanout_hidden_to_output'],\n",
        "    },\n",
        "\n",
        "    # Pre-computed energy proxies (for convenience)\n",
        "    \"energy_proxies\": {\n",
        "        \"dense_ops_per_inference\": dense_ops_per_inference,\n",
        "        \"event_syn_per_inference\": event_syn_per_inference,\n",
        "        \"sparsity_ratio\": event_syn_per_inference / dense_ops_per_inference,\n",
        "        \"formula_event_syn\": \"S_in * fanout_in_h + S_hidden * fanout_h_out\",\n",
        "        \"formula_dense\": \"N_synapses * TIME_WINDOW\",\n",
        "    },\n",
        "\n",
        "    # For optimizer resume (optional)\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "}\n",
        "\n",
        "# Save\n",
        "torch.save(checkpoint, CHECKPOINT_PATH)\n",
        "\n",
        "print(f\"Checkpoint saved: {CHECKPOINT_PATH}\")\n",
        "print(f\"\\nSchema: {checkpoint['schema_version']}\")\n",
        "print(f\"\\n\" + \"-\"*60)\n",
        "print(\"CHECKPOINT CONTENTS SUMMARY\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "print(f\"\\n[ARCHITECTURE]\")\n",
        "print(f\"  {N_INPUT} → {N_HIDDEN} → {N_OUTPUT}\")\n",
        "print(f\"  Synapses: {n_synapses:,}\")\n",
        "print(f\"  Time window: {TIME_WINDOW} steps\")\n",
        "\n",
        "print(f\"\\n[TRAINING]\")\n",
        "print(f\"  Samples: {total_training_samples:,}\")\n",
        "print(f\"  Epochs: {epochs_trained}\")\n",
        "print(f\"  Time: {training_time:.1f}s\")\n",
        "\n",
        "print(f\"\\n[PERFORMANCE]\")\n",
        "print(f\"  Best val accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"  Test accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "print(f\"\\n[TRAINING SPIKES - for Training Energy]\")\n",
        "print(f\"  Total input:  {training_spikes['input']:,.0f}\")\n",
        "print(f\"  Total hidden: {training_spikes['hidden']:,.0f}\")\n",
        "print(f\"  Total output: {training_spikes['output']:,.0f}\")\n",
        "print(f\"  Input/sample:  {training_spikes['input']/total_training_samples:.1f}\")\n",
        "print(f\"  Hidden/sample: {training_spikes['hidden']/total_training_samples:.1f}\")\n",
        "\n",
        "print(f\"\\n[INFERENCE SPIKES - for Inference Energy]\")\n",
        "print(f\"  Measured on: {inference_samples_seen} samples\")\n",
        "print(f\"  Input/inference:  {inference_spikes['input']/inference_samples_seen:.2f}\")\n",
        "print(f\"  Hidden/inference: {inference_spikes['hidden']/inference_samples_seen:.2f}\")\n",
        "print(f\"  Output/inference: {inference_spikes['output']/inference_samples_seen:.2f}\")\n",
        "\n",
        "print(f\"\\n[ENERGY PROXIES]\")\n",
        "print(f\"  Dense ops/inference:      {dense_ops_per_inference:,}\")\n",
        "print(f\"  Event syn/inference:      {event_syn_per_inference:,.2f}\")\n",
        "print(f\"  Sparsity ratio:           {event_syn_per_inference/dense_ops_per_inference:.4f}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"CHECKPOINT COMPLETE - READY FOR ENERGY ANALYSIS\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "cell_9_save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 10: Verify Checkpoint (Optional)\n",
        "# =============================================================================\n",
        "\n",
        "def inspect_surrogate_checkpoint(filepath):\n",
        "    \"\"\"Load and display all checkpoint contents.\"\"\"\n",
        "    ckpt = torch.load(filepath, map_location='cpu', weights_only=False)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"CHECKPOINT: {filepath}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    print(f\"\\n[METADATA]\")\n",
        "    print(f\"  Schema: {ckpt.get('schema_version', 'unknown')}\")\n",
        "    print(f\"  Timestamp: {ckpt.get('timestamp', 'unknown')}\")\n",
        "\n",
        "    print(f\"\\n[ARCHITECTURE]\")\n",
        "    print(f\"  Input:  {ckpt.get('n_input', 'N/A')}\")\n",
        "    print(f\"  Hidden: {ckpt.get('n_hidden', 'N/A')}\")\n",
        "    print(f\"  Output: {ckpt.get('n_output', 'N/A')}\")\n",
        "\n",
        "    enc = ckpt.get('encoding', {})\n",
        "    print(f\"\\n[ENCODING]\")\n",
        "    print(f\"  Type: {enc.get('type', 'N/A')}\")\n",
        "    print(f\"  Time window: {enc.get('time_window', 'N/A')}\")\n",
        "    print(f\"  Max rate: {enc.get('max_rate_hz', 'N/A')} Hz\")\n",
        "\n",
        "    tr = ckpt.get('training_results', {})\n",
        "    print(f\"\\n[TRAINING RESULTS]\")\n",
        "    print(f\"  Epochs: {tr.get('epochs_trained', 'N/A')}\")\n",
        "    print(f\"  Samples: {tr.get('total_samples', 'N/A'):,}\")\n",
        "    print(f\"  Time: {tr.get('training_time_seconds', 0):.1f}s\")\n",
        "    print(f\"  Best val acc: {tr.get('best_val_accuracy', 'N/A'):.2f}%\")\n",
        "\n",
        "    print(f\"\\n[TEST ACCURACY]\")\n",
        "    print(f\"  {ckpt.get('test_accuracy', 'N/A'):.2f}%\")\n",
        "\n",
        "    ts = ckpt.get('training_spikes', {})\n",
        "    print(f\"\\n[TRAINING SPIKES]\")\n",
        "    print(f\"  Total input:   {ts.get('total_input', 0):,.0f}\")\n",
        "    print(f\"  Total hidden:  {ts.get('total_hidden', 0):,.0f}\")\n",
        "    print(f\"  Total output:  {ts.get('total_output', 0):,.0f}\")\n",
        "    print(f\"  Input/sample:  {ts.get('input_per_sample', 0):.1f}\")\n",
        "    print(f\"  Hidden/sample: {ts.get('hidden_per_sample', 0):.1f}\")\n",
        "\n",
        "    inf = ckpt.get('inference_spikes', {})\n",
        "    print(f\"\\n[INFERENCE SPIKES]\")\n",
        "    print(f\"  N samples:          {inf.get('n_samples', 'N/A')}\")\n",
        "    print(f\"  Input/inference:    {inf.get('input_per_inference', 0):.2f}\")\n",
        "    print(f\"  Hidden/inference:   {inf.get('hidden_per_inference', 0):.2f}\")\n",
        "    print(f\"  Output/inference:   {inf.get('output_per_inference', 0):.2f}\")\n",
        "\n",
        "    conn = ckpt.get('connectivity', {})\n",
        "    print(f\"\\n[CONNECTIVITY]\")\n",
        "    print(f\"  Syn input→hidden:  {conn.get('n_syn_input_hidden', 'N/A'):,}\")\n",
        "    print(f\"  Syn hidden→output: {conn.get('n_syn_hidden_output', 'N/A'):,}\")\n",
        "    print(f\"  Total synapses:    {conn.get('n_synapses_total', 'N/A'):,}\")\n",
        "    print(f\"  Fanout in→hidden:  {conn.get('fanout_input_to_hidden', 'N/A')}\")\n",
        "    print(f\"  Fanout hidden→out: {conn.get('fanout_hidden_to_output', 'N/A')}\")\n",
        "\n",
        "    ep = ckpt.get('energy_proxies', {})\n",
        "    print(f\"\\n[ENERGY PROXIES]\")\n",
        "    print(f\"  Dense ops/inference:  {ep.get('dense_ops_per_inference', 0):,}\")\n",
        "    print(f\"  Event syn/inference:  {ep.get('event_syn_per_inference', 0):,.2f}\")\n",
        "    print(f\"  Sparsity ratio:       {ep.get('sparsity_ratio', 0):.6f}\")\n",
        "\n",
        "    return ckpt\n",
        "\n",
        "\n",
        "# Verify the saved checkpoint\n",
        "_ = inspect_surrogate_checkpoint(CHECKPOINT_PATH)"
      ],
      "metadata": {
        "id": "cell_10_verify"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Checkpoint Schema Reference\n",
        "\n",
        "The saved checkpoint (`surrogate_snn_checkpoint.pth`) contains:\n",
        "\n",
        "### For Training Energy Analysis\n",
        "```python\n",
        "ckpt['training_spikes']['total_input']      # Total input spikes during training\n",
        "ckpt['training_spikes']['total_hidden']     # Total hidden layer spikes\n",
        "ckpt['training_spikes']['total_output']     # Total output layer spikes\n",
        "ckpt['training_spikes']['input_per_sample'] # Average per training sample\n",
        "ckpt['training_spikes']['total_samples']    # Number of training samples\n",
        "ckpt['training_results']['training_time_seconds']\n",
        "```\n",
        "\n",
        "### For Inference Energy Analysis\n",
        "```python\n",
        "ckpt['inference_spikes']['input_per_inference']   # Avg input spikes per test image\n",
        "ckpt['inference_spikes']['hidden_per_inference']  # Avg hidden spikes per test image\n",
        "ckpt['inference_spikes']['output_per_inference']  # Avg output spikes per test image\n",
        "ckpt['inference_spikes']['n_samples']             # Number measured\n",
        "```\n",
        "\n",
        "### For Synaptic Event Calculations\n",
        "```python\n",
        "ckpt['connectivity']['n_syn_input_hidden']       # 784 * 400 = 313,600\n",
        "ckpt['connectivity']['n_syn_hidden_output']      # 400 * 10 = 4,000\n",
        "ckpt['connectivity']['fanout_input_to_hidden']   # 400\n",
        "ckpt['connectivity']['fanout_hidden_to_output']  # 10\n",
        "```\n",
        "\n",
        "### Pre-computed Proxies\n",
        "```python\n",
        "ckpt['energy_proxies']['dense_ops_per_inference']   # N_syn * T\n",
        "ckpt['energy_proxies']['event_syn_per_inference']   # S_in*400 + S_hid*10\n",
        "ckpt['energy_proxies']['sparsity_ratio']            # event/dense\n",
        "```\n",
        "\n",
        "### Performance\n",
        "```python\n",
        "ckpt['test_accuracy']                         # Final test accuracy\n",
        "ckpt['training_results']['best_val_accuracy'] # Best validation accuracy\n",
        "```"
      ],
      "metadata": {
        "id": "cell_11_docs"
      }
    }
  ]
}
