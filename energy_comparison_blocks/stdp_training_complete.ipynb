{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# STDP Network Training with Complete Energy Tracking\n",
        "\n",
        "This notebook provides:\n",
        "1. **Seamless resume** - Training can be paused and resumed without losing any data\n",
        "2. **Complete spike tracking** - Both training and inference spikes are properly accumulated\n",
        "3. **Energy-ready checkpoints** - All data needed for energy analysis is saved\n",
        "\n",
        "## Workflow\n",
        "1. Run Cells 1-4 (Setup, Encoder, Network, Save/Load functions)\n",
        "2. Run Cell 5 to either start fresh or resume from checkpoint\n",
        "3. Training automatically saves checkpoint with all spike data\n",
        "4. Evaluation (receptive fields, accuracy, confusion matrix) runs automatically\n",
        "5. To continue training, just run Cell 5 again"
      ],
      "metadata": {
        "id": "header_cell"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_1_install"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Cell 1: Install Brian2 and Imports\n",
        "# =============================================================================\n",
        "\n",
        "!pip install -q git+https://github.com/brian-team/brian2.git\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import brian2 as b2\n",
        "from brian2 import *\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Brian2 config\n",
        "b2.prefs.codegen.target = 'cython'\n",
        "b2.prefs.codegen.cpp.extra_compile_args_gcc = ['-O3', '-ffast-math']\n",
        "b2.prefs.core.default_float_dtype = np.float32\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(f\"Brian2 version: {b2.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 2: MNIST Spike Encoder\n",
        "# =============================================================================\n",
        "\n",
        "class MNISTSpikeEncoder:\n",
        "    \"\"\"\n",
        "    MNIST spike encoding following Diehl & Cook (2015).\n",
        "    Pixel intensities converted to Poisson firing rates.\n",
        "    \"\"\"\n",
        "    def __init__(self, time_window=350, rest_window=150, max_firing_rate=63.75):\n",
        "        self.time_window = time_window\n",
        "        self.rest_window = rest_window\n",
        "        self.max_rate = max_firing_rate\n",
        "\n",
        "        transform = transforms.ToTensor()\n",
        "        self.train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "        self.test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "        print(f\"Loaded {len(self.train_data)} training, {len(self.test_data)} test samples\")\n",
        "        print(f\"Encoding: {max_firing_rate} Hz max rate, {time_window}ms presentation\")\n",
        "\n",
        "# Create encoder\n",
        "encoder = MNISTSpikeEncoder()\n",
        "print(\"Encoder ready!\")"
      ],
      "metadata": {
        "id": "cell_2_encoder"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 3: STDP Network Class\n",
        "# =============================================================================\n",
        "\n",
        "class BiologicalSTDPNetwork:\n",
        "    \"\"\"\n",
        "    STDP Network matching Diehl & Cook (2015).\n",
        "    Includes spike monitors for input, excitatory, and inhibitory layers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_input=784, n_excitatory=400, n_inhibitory=400):\n",
        "        self.n_input = n_input\n",
        "        self.n_exc = n_excitatory\n",
        "        self.n_inh = n_inhibitory\n",
        "\n",
        "        # Parameters from Diehl & Cook (2015)\n",
        "        self.params = {\n",
        "            'v_rest_e': -65 * mV,\n",
        "            'v_rest_i': -60 * mV,\n",
        "            'v_reset_e': -65 * mV,\n",
        "            'v_reset_i': -45 * mV,\n",
        "            'v_thresh_e': -52 * mV,\n",
        "            'v_thresh_i': -40 * mV,\n",
        "            'tau_mem_e': 100 * ms,\n",
        "            'tau_mem_i': 10 * ms,\n",
        "            'refrac_e': 5 * ms,\n",
        "            'refrac_i': 2 * ms,\n",
        "            'E_exc': 0 * mV,\n",
        "            'E_inh': -100 * mV,\n",
        "            'tau_ge': 1 * ms,\n",
        "            'tau_gi': 2 * ms,\n",
        "            'tau_pre': 20 * ms,\n",
        "            'tau_post': 20 * ms,\n",
        "            'eta_pre': 0.0001,\n",
        "            'eta_post': 0.01,\n",
        "            'mu': 0.4,\n",
        "            'x_tar': 0.4,\n",
        "            'wmax': 1.0,\n",
        "            'theta_plus': 0.05 * mV,\n",
        "            'tau_theta': 1e7 * ms,\n",
        "            'w_ei': 10.4,\n",
        "            'w_ie': 17.0,\n",
        "        }\n",
        "\n",
        "        self.weight_norm_target = 78.4\n",
        "        self.final_weights = None\n",
        "        self.training_time = 0\n",
        "        self.net = None\n",
        "\n",
        "    def build_network(self):\n",
        "        \"\"\"Construct all neurons and synapses.\"\"\"\n",
        "        # Input layer\n",
        "        self.input_group = PoissonGroup(\n",
        "            self.n_input,\n",
        "            rates=np.zeros(self.n_input) * Hz\n",
        "        )\n",
        "\n",
        "        # Excitatory neurons\n",
        "        eqs_exc = '''\n",
        "        dv/dt = (v_rest_e - v + ge*(E_exc - v) + gi*(E_inh - v)) / tau_mem_e : volt (unless refractory)\n",
        "        dge/dt = -ge / tau_ge : 1\n",
        "        dgi/dt = -gi / tau_gi : 1\n",
        "        dtheta/dt = -theta / tau_theta : volt\n",
        "        '''\n",
        "\n",
        "        self.exc_neurons = NeuronGroup(\n",
        "            self.n_exc,\n",
        "            eqs_exc,\n",
        "            threshold='v > v_thresh_e + theta',\n",
        "            reset='v = v_reset_e; theta += theta_plus',\n",
        "            refractory='refrac_e',\n",
        "            method='euler',\n",
        "            namespace=self.params\n",
        "        )\n",
        "        self.exc_neurons.v = self.params['v_rest_e']\n",
        "        self.exc_neurons.theta = 0 * mV\n",
        "\n",
        "        # Inhibitory neurons\n",
        "        eqs_inh = '''\n",
        "        dv/dt = (v_rest_i - v + ge*(E_exc - v)) / tau_mem_i : volt (unless refractory)\n",
        "        dge/dt = -ge / tau_ge : 1\n",
        "        '''\n",
        "\n",
        "        self.inh_neurons = NeuronGroup(\n",
        "            self.n_inh,\n",
        "            eqs_inh,\n",
        "            threshold='v > v_thresh_i',\n",
        "            reset='v = v_reset_i',\n",
        "            refractory='refrac_i',\n",
        "            method='euler',\n",
        "            namespace=self.params\n",
        "        )\n",
        "        self.inh_neurons.v = self.params['v_rest_i']\n",
        "\n",
        "        # STDP synapses: Input -> Excitatory\n",
        "        stdp_model = '''\n",
        "        w : 1\n",
        "        dApre/dt = -Apre / tau_pre : 1 (event-driven)\n",
        "        dApost/dt = -Apost / tau_post : 1 (event-driven)\n",
        "        '''\n",
        "\n",
        "        on_pre = '''\n",
        "        ge_post += w\n",
        "        Apre += 1.0\n",
        "        w = clip(w - eta_pre * Apost * (w ** mu), 0, wmax)\n",
        "        '''\n",
        "\n",
        "        on_post = '''\n",
        "        Apost += 1.0\n",
        "        w = clip(w + eta_post * (Apre - x_tar) * (clip(wmax - w, 0, wmax) ** mu), 0, wmax)\n",
        "        '''\n",
        "\n",
        "        self.syn_input_exc = Synapses(\n",
        "            self.input_group,\n",
        "            self.exc_neurons,\n",
        "            model=stdp_model,\n",
        "            on_pre=on_pre,\n",
        "            on_post=on_post,\n",
        "            namespace=self.params\n",
        "        )\n",
        "        self.syn_input_exc.connect()\n",
        "        self.syn_input_exc.w = 'rand() * wmax * 0.3'\n",
        "\n",
        "        # Exc -> Inh (one-to-one)\n",
        "        self.syn_exc_inh = Synapses(\n",
        "            self.exc_neurons,\n",
        "            self.inh_neurons,\n",
        "            on_pre='ge_post += w_ei',\n",
        "            namespace=self.params\n",
        "        )\n",
        "        self.syn_exc_inh.connect('i == j')\n",
        "\n",
        "        # Inh -> Exc (lateral inhibition, excluding self)\n",
        "        self.syn_inh_exc = Synapses(\n",
        "            self.inh_neurons,\n",
        "            self.exc_neurons,\n",
        "            on_pre='gi_post += w_ie',\n",
        "            namespace=self.params\n",
        "        )\n",
        "        self.syn_inh_exc.connect('i != j')\n",
        "\n",
        "        # SPIKE MONITORS - Critical for energy analysis!\n",
        "        self.spike_mon_in = SpikeMonitor(self.input_group)\n",
        "        self.spike_mon_exc = SpikeMonitor(self.exc_neurons)\n",
        "        self.spike_mon_inh = SpikeMonitor(self.inh_neurons)\n",
        "\n",
        "        # Assemble network\n",
        "        self.net = Network(\n",
        "            self.input_group,\n",
        "            self.exc_neurons,\n",
        "            self.inh_neurons,\n",
        "            self.syn_input_exc,\n",
        "            self.syn_exc_inh,\n",
        "            self.syn_inh_exc,\n",
        "            self.spike_mon_in,\n",
        "            self.spike_mon_exc,\n",
        "            self.spike_mon_inh\n",
        "        )\n",
        "\n",
        "        self.normalize_weights()\n",
        "        print(f\"Network built: {self.n_input} input → {self.n_exc} exc ↔ {self.n_inh} inh\")\n",
        "        print(f\"Spike monitors: input, exc, inh (all active)\")\n",
        "\n",
        "    def normalize_weights(self):\n",
        "        \"\"\"Divisive weight normalization.\"\"\"\n",
        "        w = np.zeros((self.n_input, self.n_exc))\n",
        "        w[self.syn_input_exc.i[:], self.syn_input_exc.j[:]] = self.syn_input_exc.w[:]\n",
        "        col_sums = w.sum(axis=0)\n",
        "        col_sums[col_sums == 0] = 1\n",
        "        w = w * (self.weight_norm_target / col_sums)\n",
        "        w = np.clip(w, 0, self.params['wmax'])\n",
        "        self.syn_input_exc.w[:] = w[self.syn_input_exc.i[:], self.syn_input_exc.j[:]]\n",
        "\n",
        "    def present_sample(self, rates_hz, min_spikes=5, max_attempts=5):\n",
        "        \"\"\"Present a sample with adaptive rate boosting.\"\"\"\n",
        "        rate_boost = 0.0\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            n_before = self.spike_mon_exc.num_spikes\n",
        "            self.input_group.rates = (rates_hz + rate_boost * Hz)\n",
        "            self.net.run(350 * ms)\n",
        "            n_after = self.spike_mon_exc.num_spikes\n",
        "            n_spikes = n_after - n_before\n",
        "\n",
        "            # Rest period\n",
        "            self.input_group.rates = np.zeros(self.n_input) * Hz\n",
        "            self.net.run(150 * ms)\n",
        "\n",
        "            if n_spikes >= min_spikes:\n",
        "                return n_spikes, attempt + 1\n",
        "            rate_boost += 32.0\n",
        "\n",
        "        return n_spikes, max_attempts\n",
        "\n",
        "    def train(self, encoder, n_samples, normalize_interval=20, print_interval=100):\n",
        "        \"\"\"Train the network.\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training STDP Network\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Samples: {n_samples}\")\n",
        "\n",
        "        if self.net is None:\n",
        "            self.build_network()\n",
        "\n",
        "        start_time = time.time()\n",
        "        total_spikes = 0\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            img, label = encoder.train_data[i % len(encoder.train_data)]\n",
        "            rates = img.numpy().flatten() * encoder.max_rate * Hz\n",
        "            n_spikes, attempts = self.present_sample(rates)\n",
        "            total_spikes += n_spikes\n",
        "\n",
        "            if (i + 1) % normalize_interval == 0:\n",
        "                self.normalize_weights()\n",
        "\n",
        "            if (i + 1) % print_interval == 0 or (i + 1) <= 3:\n",
        "                elapsed = time.time() - start_time\n",
        "                avg_time = elapsed / (i + 1)\n",
        "                remaining = avg_time * (n_samples - i - 1)\n",
        "                print(f\"[{i+1:6d}/{n_samples}] \"\n",
        "                      f\"spikes: {total_spikes/(i+1):.1f}, \"\n",
        "                      f\"θ: {np.mean(self.exc_neurons.theta/mV):.2f}mV, \"\n",
        "                      f\"ETA: {remaining/60:.1f}min\")\n",
        "\n",
        "        self.training_time = time.time() - start_time\n",
        "        self._save_final_state()\n",
        "        print(f\"\\nTraining complete! Time: {self.training_time/60:.1f} min\")\n",
        "\n",
        "    def _save_final_state(self):\n",
        "        \"\"\"Cache final state for analysis.\"\"\"\n",
        "        w = np.zeros((self.n_input, self.n_exc))\n",
        "        w[self.syn_input_exc.i[:], self.syn_input_exc.j[:]] = self.syn_input_exc.w[:]\n",
        "        self.final_weights = {\n",
        "            'weights': w,\n",
        "            'theta': np.array(self.exc_neurons.theta / mV),\n",
        "        }\n",
        "\n",
        "print(\"BiologicalSTDPNetwork class defined!\")"
      ],
      "metadata": {
        "id": "cell_3_network"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 4: Save/Load Functions with Spike Accumulation\n",
        "# =============================================================================\n",
        "\n",
        "def save_complete_checkpoint(\n",
        "    network,\n",
        "    encoder,\n",
        "    filepath,\n",
        "    total_training_samples,\n",
        "    previous_checkpoint=None,\n",
        "    inference_accuracy=None,\n",
        "    neuron_labels=None,\n",
        "    n_inference_samples=500,\n",
        "    notes=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Save complete checkpoint with:\n",
        "    - Accumulated training spikes (across sessions)\n",
        "    - Inference spikes (measured on test data)\n",
        "    - All data needed for energy analysis\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Saving Complete Checkpoint\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # Part 1: Accumulate TRAINING spikes\n",
        "    # =========================================================================\n",
        "    current_train_in = int(network.spike_mon_in.num_spikes)\n",
        "    current_train_exc = int(network.spike_mon_exc.num_spikes)\n",
        "    current_train_inh = int(network.spike_mon_inh.num_spikes)\n",
        "\n",
        "    if previous_checkpoint is not None:\n",
        "        prev_train = previous_checkpoint.get(\"training_spikes\", {})\n",
        "        prev_in = int(prev_train.get(\"total_input\", 0) or 0)\n",
        "        prev_exc = int(prev_train.get(\"total_exc\", 0) or 0)\n",
        "        prev_inh = int(prev_train.get(\"total_inh\", 0) or 0)\n",
        "        prev_time = float(previous_checkpoint.get(\"training_time_seconds\", 0) or 0)\n",
        "        print(f\"  Previous training spikes: in={prev_in}, exc={prev_exc}, inh={prev_inh}\")\n",
        "    else:\n",
        "        prev_in = prev_exc = prev_inh = 0\n",
        "        prev_time = 0\n",
        "\n",
        "    acc_train_in = current_train_in + prev_in\n",
        "    acc_train_exc = current_train_exc + prev_exc\n",
        "    acc_train_inh = current_train_inh + prev_inh\n",
        "    acc_time = network.training_time + prev_time\n",
        "\n",
        "    print(f\"  Current session spikes: in={current_train_in}, exc={current_train_exc}, inh={current_train_inh}\")\n",
        "    print(f\"  ACCUMULATED training:   in={acc_train_in}, exc={acc_train_exc}, inh={acc_train_inh}\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # Part 2: Measure INFERENCE spikes (on test data)\n",
        "    # =========================================================================\n",
        "    print(f\"\\n  Measuring inference spikes on {n_inference_samples} test samples...\")\n",
        "\n",
        "    # Record current counts (after training)\n",
        "    pre_inf_in = int(network.spike_mon_in.num_spikes)\n",
        "    pre_inf_exc = int(network.spike_mon_exc.num_spikes)\n",
        "    pre_inf_inh = int(network.spike_mon_inh.num_spikes)\n",
        "\n",
        "    inf_start = time.time()\n",
        "    for i in range(n_inference_samples):\n",
        "        img, label = encoder.test_data[i % len(encoder.test_data)]\n",
        "        rates = img.numpy().flatten() * encoder.max_rate * Hz\n",
        "        network.input_group.rates = rates\n",
        "        network.net.run(350 * ms)\n",
        "        network.input_group.rates = np.zeros(network.n_input) * Hz\n",
        "        network.net.run(150 * ms)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"    [{i+1}/{n_inference_samples}]\")\n",
        "\n",
        "    inf_time = time.time() - inf_start\n",
        "\n",
        "    # Compute inference-only spikes\n",
        "    inf_in = int(network.spike_mon_in.num_spikes) - pre_inf_in\n",
        "    inf_exc = int(network.spike_mon_exc.num_spikes) - pre_inf_exc\n",
        "    inf_inh = int(network.spike_mon_inh.num_spikes) - pre_inf_inh\n",
        "\n",
        "    print(f\"  Inference spikes: in={inf_in}, exc={inf_exc}, inh={inf_inh}\")\n",
        "    print(f\"  Spikes/inference: in={inf_in/n_inference_samples:.1f}, \"\n",
        "          f\"exc={inf_exc/n_inference_samples:.1f}, inh={inf_inh/n_inference_samples:.1f}\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # Part 3: Extract weights and theta\n",
        "    # =========================================================================\n",
        "    w_dense = np.zeros((network.n_input, network.n_exc), dtype=np.float32)\n",
        "    w_dense[network.syn_input_exc.i[:], network.syn_input_exc.j[:]] = \\\n",
        "        np.asarray(network.syn_input_exc.w[:], dtype=np.float32)\n",
        "\n",
        "    theta_arr = np.array(network.exc_neurons.theta / mV, dtype=np.float32)\n",
        "\n",
        "    # =========================================================================\n",
        "    # Part 4: Build and save checkpoint\n",
        "    # =========================================================================\n",
        "    checkpoint = {\n",
        "        \"schema_version\": \"complete_v1\",\n",
        "        \"saved_at_unix\": time.time(),\n",
        "        \"notes\": notes,\n",
        "\n",
        "        # Architecture\n",
        "        \"n_input\": int(network.n_input),\n",
        "        \"n_exc\": int(network.n_exc),\n",
        "        \"n_inh\": int(network.n_inh),\n",
        "\n",
        "        # Learned state\n",
        "        \"weights\": w_dense,\n",
        "        \"theta\": theta_arr,\n",
        "\n",
        "        # Training metadata\n",
        "        \"training_samples\": int(total_training_samples),\n",
        "        \"training_time_seconds\": acc_time,\n",
        "\n",
        "        # TRAINING SPIKES (accumulated across all sessions)\n",
        "        \"training_spikes\": {\n",
        "            \"total_input\": acc_train_in,\n",
        "            \"total_exc\": acc_train_exc,\n",
        "            \"total_inh\": acc_train_inh,\n",
        "            \"input_per_sample\": acc_train_in / total_training_samples,\n",
        "            \"exc_per_sample\": acc_train_exc / total_training_samples,\n",
        "            \"inh_per_sample\": acc_train_inh / total_training_samples,\n",
        "        },\n",
        "\n",
        "        # INFERENCE SPIKES (from test data measurement)\n",
        "        \"inference_spikes\": {\n",
        "            \"n_samples\": n_inference_samples,\n",
        "            \"total_input\": inf_in,\n",
        "            \"total_exc\": inf_exc,\n",
        "            \"total_inh\": inf_inh,\n",
        "            \"input_per_inference\": inf_in / n_inference_samples,\n",
        "            \"exc_per_inference\": inf_exc / n_inference_samples,\n",
        "            \"inh_per_inference\": inf_inh / n_inference_samples,\n",
        "            \"measurement_time_seconds\": inf_time,\n",
        "        },\n",
        "\n",
        "        # For backward compatibility with energy_comparison_blocks.ipynb\n",
        "        \"total_input_spikes\": acc_train_in,\n",
        "        \"total_exc_spikes\": acc_train_exc,\n",
        "        \"total_inh_spikes\": acc_train_inh,\n",
        "\n",
        "        # Connectivity\n",
        "        \"connectivity\": {\n",
        "            \"n_syn_input_exc\": len(network.syn_input_exc),\n",
        "            \"n_syn_exc_inh\": len(network.syn_exc_inh),\n",
        "            \"n_syn_inh_exc\": len(network.syn_inh_exc),\n",
        "            \"fanout_input_to_exc\": int(network.n_exc),\n",
        "            \"fanout_exc_to_inh\": 1,\n",
        "            \"fanout_inh_to_exc\": max(network.n_exc - 1, 0),\n",
        "        },\n",
        "\n",
        "        # Evaluation results\n",
        "        \"inference_accuracy\": inference_accuracy,\n",
        "        \"neuron_labels\": neuron_labels,\n",
        "    }\n",
        "\n",
        "    with open(filepath, \"wb\") as f:\n",
        "        pickle.dump(checkpoint, f)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"✓ Checkpoint saved: {filepath}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Training samples:     {total_training_samples}\")\n",
        "    print(f\"  Training time:        {acc_time/60:.1f} min\")\n",
        "    print(f\"  Train spikes/sample:  in={acc_train_in/total_training_samples:.1f}, \"\n",
        "          f\"exc={acc_train_exc/total_training_samples:.1f}\")\n",
        "    print(f\"  Inf spikes/inference: in={inf_in/n_inference_samples:.1f}, \"\n",
        "          f\"exc={inf_exc/n_inference_samples:.1f}\")\n",
        "    if inference_accuracy is not None:\n",
        "        print(f\"  Accuracy:             {inference_accuracy:.2f}%\")\n",
        "\n",
        "    return checkpoint\n",
        "\n",
        "\n",
        "def load_checkpoint(filepath, verbose=True):\n",
        "    \"\"\"\n",
        "    Load checkpoint and restore network.\n",
        "    Returns (network, checkpoint) for seamless resume.\n",
        "    \"\"\"\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        checkpoint = pickle.load(f)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Loading checkpoint: {filepath}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"  Schema: {checkpoint.get('schema_version', 'unknown')}\")\n",
        "        print(f\"  Training samples: {checkpoint.get('training_samples', 'unknown')}\")\n",
        "\n",
        "    # Create fresh network with correct Brian2 units\n",
        "    n_input = int(checkpoint.get(\"n_input\", 784))\n",
        "    n_exc = int(checkpoint.get(\"n_exc\", 400))\n",
        "    n_inh = int(checkpoint.get(\"n_inh\", 400))\n",
        "\n",
        "    network = BiologicalSTDPNetwork(\n",
        "        n_input=n_input,\n",
        "        n_excitatory=n_exc,\n",
        "        n_inhibitory=n_inh\n",
        "    )\n",
        "    network.build_network()\n",
        "\n",
        "    # Restore weights\n",
        "    w = checkpoint.get(\"weights\")\n",
        "    if w is not None:\n",
        "        network.syn_input_exc.w[:] = w[network.syn_input_exc.i[:], network.syn_input_exc.j[:]]\n",
        "        if verbose:\n",
        "            print(f\"  ✓ Weights restored: {w.shape}\")\n",
        "\n",
        "    # Restore theta\n",
        "    theta = checkpoint.get(\"theta\")\n",
        "    if theta is not None:\n",
        "        network.exc_neurons.theta = np.asarray(theta) * mV\n",
        "        if verbose:\n",
        "            print(f\"  ✓ Theta restored: mean={np.mean(theta):.2f} mV\")\n",
        "\n",
        "    # Restore training time\n",
        "    network.training_time = float(checkpoint.get(\"training_time_seconds\", 0))\n",
        "\n",
        "    # Cache for analysis functions\n",
        "    network.final_weights = {\n",
        "        'weights': w,\n",
        "        'theta': theta,\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        train_spikes = checkpoint.get(\"training_spikes\", {})\n",
        "        inf_spikes = checkpoint.get(\"inference_spikes\", {})\n",
        "        print(f\"  Training spikes (accumulated): \"\n",
        "              f\"in={train_spikes.get('total_input', 'N/A')}, \"\n",
        "              f\"exc={train_spikes.get('total_exc', 'N/A')}\")\n",
        "        print(f\"  Inference spikes/inf: \"\n",
        "              f\"in={inf_spikes.get('input_per_inference', 'N/A'):.1f}, \"\n",
        "              f\"exc={inf_spikes.get('exc_per_inference', 'N/A'):.1f}\")\n",
        "        print(f\"  Ready to resume training!\")\n",
        "\n",
        "    return network, checkpoint\n",
        "\n",
        "\n",
        "print(\"Save/Load functions defined!\")"
      ],
      "metadata": {
        "id": "cell_4_saveload"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 5: Evaluation Functions\n",
        "# =============================================================================\n",
        "\n",
        "def assign_neuron_labels(network, encoder, n_samples=10000):\n",
        "    \"\"\"Assign digit labels to neurons based on response.\"\"\"\n",
        "    print(f\"\\nAssigning neuron labels ({n_samples} samples)...\")\n",
        "\n",
        "    weights = network.final_weights['weights']\n",
        "    neuron_responses = np.zeros((network.n_exc, 10))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        img, label = encoder.train_data[i]\n",
        "        flat_img = img.numpy().flatten()\n",
        "        response = flat_img @ weights\n",
        "\n",
        "        # Winner-take-all: top-k neurons vote\n",
        "        k = 10\n",
        "        top_k = np.argsort(response)[-k:]\n",
        "        for idx in top_k:\n",
        "            neuron_responses[idx, label] += response[idx]\n",
        "\n",
        "        if (i + 1) % 2000 == 0:\n",
        "            print(f\"  [{i+1}/{n_samples}]\")\n",
        "\n",
        "    neuron_labels = np.argmax(neuron_responses, axis=1)\n",
        "    max_responses = np.max(neuron_responses, axis=1)\n",
        "    sum_responses = np.sum(neuron_responses, axis=1)\n",
        "    sum_responses[sum_responses == 0] = 1\n",
        "    confidence = max_responses / sum_responses\n",
        "\n",
        "    # Report distribution\n",
        "    print(f\"\\n  Label distribution:\")\n",
        "    for digit in range(10):\n",
        "        count = np.sum(neuron_labels == digit)\n",
        "        print(f\"    Digit {digit}: {count} neurons ({100*count/len(neuron_labels):.1f}%)\")\n",
        "\n",
        "    return neuron_labels, confidence\n",
        "\n",
        "\n",
        "def test_accuracy(network, encoder, neuron_labels, n_samples=10000):\n",
        "    \"\"\"Test accuracy using neuron labels.\"\"\"\n",
        "    print(f\"\\nTesting accuracy ({n_samples} samples)...\")\n",
        "\n",
        "    weights = network.final_weights['weights']\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        img, label = encoder.test_data[i % len(encoder.test_data)]\n",
        "        flat_img = img.numpy().flatten()\n",
        "        response = flat_img @ weights\n",
        "\n",
        "        # Voting\n",
        "        votes = np.zeros(10)\n",
        "        k = 10\n",
        "        top_k = np.argsort(response)[-k:]\n",
        "        for idx in top_k:\n",
        "            votes[neuron_labels[idx]] += response[idx]\n",
        "\n",
        "        predictions.append(np.argmax(votes))\n",
        "        true_labels.append(label)\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    true_labels = np.array(true_labels)\n",
        "    accuracy = 100.0 * np.mean(predictions == true_labels)\n",
        "\n",
        "    print(f\"\\n  Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy, predictions, true_labels\n",
        "\n",
        "\n",
        "def plot_receptive_fields(network, n_show=100):\n",
        "    \"\"\"Plot learned receptive fields.\"\"\"\n",
        "    weights = network.final_weights['weights']\n",
        "    n_neurons = min(n_show, weights.shape[1])\n",
        "    grid_size = int(np.ceil(np.sqrt(n_neurons)))\n",
        "\n",
        "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
        "\n",
        "    for i in range(grid_size * grid_size):\n",
        "        ax = axes[i // grid_size, i % grid_size]\n",
        "        if i < n_neurons:\n",
        "            rf = weights[:, i].reshape(28, 28)\n",
        "            ax.imshow(rf, cmap='hot', interpolation='nearest')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.suptitle('Learned Receptive Fields', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(true_labels, predictions, accuracy):\n",
        "    \"\"\"Plot confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=range(10), yticklabels=range(10))\n",
        "    plt.xlabel('Predicted', fontsize=12)\n",
        "    plt.ylabel('True', fontsize=12)\n",
        "    plt.title(f'Confusion Matrix (Accuracy: {accuracy:.1f}%)', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nPer-class accuracy:\")\n",
        "    for digit in range(10):\n",
        "        mask = true_labels == digit\n",
        "        class_acc = 100.0 * np.mean(predictions[mask] == digit)\n",
        "        print(f\"  Digit {digit}: {class_acc:.1f}%\")\n",
        "\n",
        "\n",
        "print(\"Evaluation functions defined!\")"
      ],
      "metadata": {
        "id": "cell_5_evaluation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 6: MAIN TRAINING/RESUME CELL\n",
        "# =============================================================================\n",
        "# \n",
        "# CONFIGURE THESE PARAMETERS:\n",
        "#\n",
        "\n",
        "CHECKPOINT_PATH = \"stdp_checkpoint.pkl\"  # Checkpoint file path\n",
        "SAMPLES_THIS_SESSION = 1000              # How many samples to train this session\n",
        "N_INFERENCE_SAMPLES = 500                # Samples for inference spike measurement\n",
        "\n",
        "#\n",
        "# =============================================================================\n",
        "\n",
        "# Start fresh scope\n",
        "b2.start_scope()\n",
        "\n",
        "# Check if resuming or starting fresh\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    print(\"Found existing checkpoint - RESUMING\")\n",
        "    stdp_net, prev_ckpt = load_checkpoint(CHECKPOINT_PATH)\n",
        "    previous_samples = prev_ckpt.get(\"training_samples\", 0)\n",
        "else:\n",
        "    print(\"No checkpoint found - STARTING FRESH\")\n",
        "    stdp_net = BiologicalSTDPNetwork()\n",
        "    stdp_net.build_network()\n",
        "    prev_ckpt = None\n",
        "    previous_samples = 0\n",
        "\n",
        "# Train\n",
        "stdp_net.train(\n",
        "    encoder=encoder,\n",
        "    n_samples=SAMPLES_THIS_SESSION,\n",
        "    normalize_interval=20,\n",
        "    print_interval=100\n",
        ")\n",
        "\n",
        "total_samples = previous_samples + SAMPLES_THIS_SESSION\n",
        "print(f\"\\nTotal training samples: {total_samples}\")\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "plot_receptive_fields(stdp_net, n_show=100)\n",
        "\n",
        "neuron_labels, confidence = assign_neuron_labels(stdp_net, encoder, n_samples=10000)\n",
        "accuracy, predictions, true_labels = test_accuracy(stdp_net, encoder, neuron_labels, n_samples=10000)\n",
        "\n",
        "plot_confusion_matrix(true_labels, predictions, accuracy)\n",
        "\n",
        "# Save complete checkpoint\n",
        "save_complete_checkpoint(\n",
        "    network=stdp_net,\n",
        "    encoder=encoder,\n",
        "    filepath=CHECKPOINT_PATH,\n",
        "    total_training_samples=total_samples,\n",
        "    previous_checkpoint=prev_ckpt,\n",
        "    inference_accuracy=accuracy,\n",
        "    neuron_labels=neuron_labels,\n",
        "    n_inference_samples=N_INFERENCE_SAMPLES,\n",
        "    notes=f\"Session: {SAMPLES_THIS_SESSION} samples, Total: {total_samples}\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SESSION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"To continue training, just run this cell again!\")\n",
        "print(f\"Checkpoint: {CHECKPOINT_PATH}\")"
      ],
      "metadata": {
        "id": "cell_6_main"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cell 7: View Checkpoint Contents (Optional)\n",
        "# =============================================================================\n",
        "\n",
        "def inspect_checkpoint(filepath):\n",
        "    \"\"\"Display all checkpoint contents for verification.\"\"\"\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        ckpt = pickle.load(f)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"CHECKPOINT CONTENTS: {filepath}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    print(f\"\\n[METADATA]\")\n",
        "    print(f\"  Schema:           {ckpt.get('schema_version', 'unknown')}\")\n",
        "    print(f\"  Training samples: {ckpt.get('training_samples', 'N/A')}\")\n",
        "    print(f\"  Training time:    {ckpt.get('training_time_seconds', 0)/60:.1f} min\")\n",
        "    print(f\"  Accuracy:         {ckpt.get('inference_accuracy', 'N/A')}%\")\n",
        "\n",
        "    print(f\"\\n[ARCHITECTURE]\")\n",
        "    print(f\"  Input:  {ckpt.get('n_input', 'N/A')}\")\n",
        "    print(f\"  Exc:    {ckpt.get('n_exc', 'N/A')}\")\n",
        "    print(f\"  Inh:    {ckpt.get('n_inh', 'N/A')}\")\n",
        "\n",
        "    print(f\"\\n[TRAINING SPIKES - for Training Energy Analysis]\")\n",
        "    ts = ckpt.get(\"training_spikes\", {})\n",
        "    print(f\"  Total input:     {ts.get('total_input', 'N/A'):,}\")\n",
        "    print(f\"  Total exc:       {ts.get('total_exc', 'N/A'):,}\")\n",
        "    print(f\"  Total inh:       {ts.get('total_inh', 'N/A'):,}\")\n",
        "    print(f\"  Input/sample:    {ts.get('input_per_sample', 'N/A'):.1f}\")\n",
        "    print(f\"  Exc/sample:      {ts.get('exc_per_sample', 'N/A'):.1f}\")\n",
        "\n",
        "    print(f\"\\n[INFERENCE SPIKES - for Inference Energy Analysis]\")\n",
        "    inf = ckpt.get(\"inference_spikes\", {})\n",
        "    print(f\"  Measured on:     {inf.get('n_samples', 'N/A')} test samples\")\n",
        "    print(f\"  Input/inference: {inf.get('input_per_inference', 'N/A'):.1f}\")\n",
        "    print(f\"  Exc/inference:   {inf.get('exc_per_inference', 'N/A'):.1f}\")\n",
        "    print(f\"  Inh/inference:   {inf.get('inh_per_inference', 'N/A'):.1f}\")\n",
        "\n",
        "    print(f\"\\n[CONNECTIVITY]\")\n",
        "    conn = ckpt.get(\"connectivity\", {})\n",
        "    print(f\"  Input→Exc synapses: {conn.get('n_syn_input_exc', 'N/A'):,}\")\n",
        "    print(f\"  Fanout input→exc:   {conn.get('fanout_input_to_exc', 'N/A')}\")\n",
        "\n",
        "    print(f\"\\n[LEARNED STATE]\")\n",
        "    w = ckpt.get(\"weights\")\n",
        "    theta = ckpt.get(\"theta\")\n",
        "    if w is not None:\n",
        "        print(f\"  Weights shape:   {w.shape}\")\n",
        "        print(f\"  Weights range:   [{w.min():.4f}, {w.max():.4f}]\")\n",
        "    if theta is not None:\n",
        "        print(f\"  Theta mean:      {np.mean(theta):.2f} mV\")\n",
        "        print(f\"  Theta range:     [{theta.min():.2f}, {theta.max():.2f}] mV\")\n",
        "\n",
        "    labels = ckpt.get(\"neuron_labels\")\n",
        "    if labels is not None:\n",
        "        print(f\"\\n[NEURON LABELS]\")\n",
        "        for digit in range(10):\n",
        "            count = np.sum(labels == digit)\n",
        "            print(f\"  Digit {digit}: {count} neurons\")\n",
        "\n",
        "    return ckpt\n",
        "\n",
        "\n",
        "# Inspect the current checkpoint\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    _ = inspect_checkpoint(CHECKPOINT_PATH)\n",
        "else:\n",
        "    print(f\"Checkpoint not found: {CHECKPOINT_PATH}\")"
      ],
      "metadata": {
        "id": "cell_7_inspect"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Energy Analysis Compatibility\n",
        "\n",
        "The checkpoint saved by this notebook contains everything needed for `energy_comparison_blocks.ipynb`:\n",
        "\n",
        "### For Training Energy (Block 3)\n",
        "- `training_spikes.total_input` - Total input spikes during training\n",
        "- `training_spikes.total_exc` - Total excitatory spikes during training\n",
        "- `training_spikes.total_inh` - Total inhibitory spikes during training\n",
        "- `training_samples` - Number of training samples\n",
        "- `training_time_seconds` - Wall-clock training time\n",
        "\n",
        "### For Inference Energy (Block 4)\n",
        "- `inference_spikes.input_per_inference` - Avg input spikes per test sample\n",
        "- `inference_spikes.exc_per_inference` - Avg exc spikes per test sample\n",
        "- `inference_spikes.inh_per_inference` - Avg inh spikes per test sample\n",
        "- `inference_spikes.n_samples` - Number of test samples measured\n",
        "\n",
        "### For Network Topology\n",
        "- `connectivity.n_syn_input_exc` - Number of Input→Exc synapses\n",
        "- `connectivity.fanout_input_to_exc` - Fanout per input neuron\n",
        "- All other connectivity metadata\n",
        "\n",
        "### Backward Compatibility\n",
        "- `total_input_spikes`, `total_exc_spikes`, `total_inh_spikes` - Legacy fields"
      ],
      "metadata": {
        "id": "cell_8_docs"
      }
    }
  ]
}
